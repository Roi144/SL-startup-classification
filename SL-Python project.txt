# Python project
### Roi Almakias 
### Alexandra Asinovskaya 

# Startup success prediction

Using the startup dataset from https://www.kaggle.com/manishkc06/startup-success-prediction.

The objective is to predict whether a startup which is currently operating turns into a success or a failure. The success of a company is defined as the event that gives the company's founders a large sum of money through the process of M&A (Merger and Acquisition) or an IPO (Initial Public Offering). A company would be considered as failed if it had to be shut down.

**About the Data**

The data contains industry trends, investment insights and individual company information. There are **48** columns/features. Some of the features are:

* agefirstfunding_year – quantitative
* agelastfunding_year – quantitative
* relationships – quantitative
* funding_rounds – quantitative
* fundingtotalusd – quantitative
* milestones – quantitative
* agefirstmilestone_year – quantitative
* agelastmilestone_year – quantitative
* state – categorical
* industry_type – categorical
* has_VC – categorical
* has_angel – categorical
* has_roundA – categorical
* has_roundB – categorical
* has_roundC – categorical
* has_roundD – categorical
* avg_participants – quantitative
* is_top500 – categorical
* status(acquired/closed) – categorical (the target variable, if a startup is ‘acquired’ by some other organization, means the startup succeed) 

# importing packages 


# In[1]:

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from datetime import datetime
from sklearn.model_selection import train_test_split as ttsp
from sklearn import feature_selection as ftr_slct
from sklearn.preprocessing import scale
import seaborn as sns; sns.set_theme()
from matplotlib.pyplot import figure
from statsmodels.stats.outliers_influence import variance_inflation_factor
from warnings import simplefilter
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import GradientBoostingClassifier as GBC
from sklearn.utils import resample


# In[2]:

startup = pd.read_csv('startup_data.csv')
startup.head()

# In[3]:

startup.shape


# In[4]:

startup.describe()

# In[5]:

startup.isnull().sum()

## We can see that there are a lot of missing values in variables in *age_first_milestone_year* and *age_last_milestone_year*. It can be because other startups didn't have milestones at all. Here we will not assume that and will get rid of this variables completely.  But we will delete variables *Unnamed: 0*,  *Unnamed: 6*, *name*, *zip_code*, *id* and *object_id*, because those are unique values that definitely will not help predictions. Also we will drop variable *closed_at* because, naturaly, it has a lot of missing values.


## Also, some of the variables we will transform to suit better for our prediction. Each variable of dates we will transform to two separate variables, one for month and one for year. Also we will get rid of variable *status*, because we have dummy variable *labels* for prediction.


# In[6]:

startup["founded_at"]=pd.to_datetime(startup["founded_at"],format="%m/%d/%Y")
startup["first_funding_at"]=pd.to_datetime(startup["first_funding_at"],format="%m/%d/%Y")
startup["last_funding_at"]=pd.to_datetime(startup["last_funding_at"],format="%m/%d/%Y")

startup["founding_year"]=startup["founded_at"].dt.year
startup["founding_month"]=startup["founded_at"].dt.month
startup["first_funding_year"]=startup["first_funding_at"].dt.year
startup["first_funding_month"]=startup["first_funding_at"].dt.month
startup["last_funding_year"]=startup["founded_at"].dt.year
startup["last_funding_month"]=startup["founded_at"].dt.month
drop_var = ['founded_at','first_funding_at','last_funding_at','status']
startup.drop(drop_var, axis=1, inplace=True)
startup.head()

# In[7]:


drop_var = ['age_first_milestone_year','age_last_milestone_year', 'Unnamed: 0',  'Unnamed: 6', 'name', 'zip_code', 'id',
            'object_id', 'closed_at','state_code','state_code.1','category_code']
startup.drop(drop_var, axis=1, inplace=True)
startup.head()

# In[8]:

startup["city"].unique().size

# In[9]:

startup["city"].value_counts().sort_values(ascending=False).head(n=20)

## We can see that we have one categorical value *city* with 221 unique values. To include all of them in the model we will have to make 221 dummy varables for each city. In order to make our data less combersome, we will use only 20 dummy variables for cities with 10 and more startups and the rest we will mark as *other*.

# In[10]:

other = startup["city"].value_counts().sort_values(ascending=False).index
startup["city"]= ["other" if startup["city"][i] not in other[0:20] else startup["city"][i] for i in startup["city"].index]

# In[11]:

startup["West"]=[1 if i<=(-110) else 0 for i in startup["longitude"]]
#startup["WCenterE"]=[1 if i>=-110 and i<-90 else 0 for i in startup["longitude"]]
startup["East"]=[1 if i>=-90 else 0 for i in startup["longitude"]]
startup["South"]=[1 if i<=37 else 0 for i in startup["latitude"]]
#startup["SCenterN"]=[1 if i>=37 and i<42 else 0 for i in startup["latitude"]]
startup["North"]=[1 if i>=42 else 0 for i in startup["latitude"]]

# In[12]:

startup.drop(["latitude","longitude"], axis=1, inplace=True)
cat_names = sorted(startup.select_dtypes(include='object'))
startup_d = pd.get_dummies(startup, columns=cat_names)
startup_d.head()

## # Visual analisys

# In[13]:

np.random.seed(0)
figure(figsize=(12, 10))
ax = sns.heatmap(startup_d.corr())

## We can see that some variables have high correlation, and it can be a problem for some models. However, this is expected, because we have variables like *city_New York* and *is_NY* for example, as it can be that most of startups in state New york are in New York city.

# In[14]:

figure(figsize=(12, 10))
splot = sns.scatterplot(data=startup_d, x="funding_total_usd", y="relationships", hue="labels")
splot.set(xscale="log")


# In[15]:

startup["category"] = startup[["is_software","is_web","is_mobile","is_enterprise","is_advertising","is_gamesvideo","is_ecommerce","is_biotech","is_consulting","is_othercategory"]].idxmax(axis=1)

# In[16]:

figure(figsize=(12, 10))
sns.countplot(x="category",hue="labels", data=startup)

## From graph above we can see that overall there are more successful prediction in almost all categories. In software there are more successful startups, but in e-commerce there are more failing ones. 

# In[17]:

figure(figsize=(12, 10))
startup["city"]= ["other" if startup["city"][i] not in other[0:10] else startup["city"][i] for i in startup["city"].index]
sns.countplot(x="city",hue="labels", data=startup)

# In[18]:


def calc_vif(X):

    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

# In[19]:

calc_vif(startup_d).sort_values(by="VIF",ascending=False).head(n=20)

## It seems that we have some predictors with high collinearity so we will drop some of them.

# In[20]:

startup_d.drop(["city_other","is_othercategory","is_otherstate","founding_year","last_funding_month","last_funding_year","first_funding_year"], axis=1, inplace=True)

## Let's see if it helped. 

# In[21]:

calc_vif(startup_d).sort_values(by="VIF",ascending=False).head(n=20)

# In[22]:

x=startup_d.drop(columns=['labels'])
y=startup_d['labels']

# In[23]:

selector = ftr_slct.VarianceThreshold(0.003)
selector.fit_transform(x).shape

## It looks like we don't have predictors with very low variance. 

# In[24]:

simplefilter(action='ignore', category=FutureWarning)

# In[25]:

# scaling the data
scaled_x = scale(x)

# In[26]:

scaled_x=pd.DataFrame(scaled_x,columns=x.columns)

# In[27]:

x_train, x_test , y_train ,y_test =ttsp(scaled_x,y,test_size=0.25)


## ### LASSO for variables selection

## We have a lot of variables, and we know that logistic regression is not very flexible, thats why to get a better prediction we will use LASSO to select variables to avoid underfitting. 

# In[28]:

sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1',solver="liblinear")) # reducing the amount of predictors with LASSO
sel_.fit(x_train, y_train)

# In[29]:

selected_feat = x_train.columns[(sel_.get_support())]
print('total features: {}'.format((x_train.shape[1])))
print('selected features: {}'.format(len(selected_feat)))
print('features with coefficients shrank to zero: {}'.format(
      np.sum(sel_.estimator_.coef_ == 0)))

# In[30]:

x_train_sel = sel_.transform(x_train)
x_test_sel = sel_.transform(x_test)
x_train_sel.shape, x_test_sel.shape

## ### Logistic Regression


# In[31]:

# using only selected variables
log_model = LogisticRegression()
log_model.fit(x_train_sel,y_train)
y_pred = log_model.predict(x_test_sel)

# In[32]:

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

# In[33]:

(tn,fp,fn,tp)

# In[34]:

acc_log = accuracy_score(y_test, y_pred)
acc_log


## # LDA

# In[35]:

lda = LinearDiscriminantAnalysis()
mod_lda = lda.fit(x_train, y_train)
y_pred=mod_lda.predict(x_test)

# In[36]:

acc_lda = accuracy_score(y_test, y_pred)
acc_lda

### K Neighbors Classifier

# In[37]:

knn = KNeighborsClassifier
cv_knn=np.array([])
k_chk=range(1,round(0.9*len(y_train)),2)
# finding k with highest accuracy through cross-validation
for k in k_chk:
    knn2=knn(n_neighbors=k)
    cv_knn=np.append(cv_knn,cross_val_score(knn2, x_train, y_train, cv=10, scoring='accuracy').mean())

# In[38]:

chosen_k=k_chk[np.min(np.where(cv_knn>=(cv_knn.max()-0.01)))]
chosen_k

# In[39]:

sns.lineplot(k_chk[1:150],cv_knn[1:150])
sns.lineplot(k_chk[1:150],cv_knn.max()-0.01)
sns.lineplot(x=np.array([chosen_k,chosen_k]),y=np.array([cv_knn.min()-0.01,cv_knn.max()+0.01]), color="0",linewidth=0)

# In[40]:

knn2=knn(n_neighbors=chosen_k)
knn2.fit(x_train,y_train)

# In[41]:

y_pred=knn2.predict(x_test)
acc_knn = accuracy_score(y_test, y_pred)
acc_knn

## ## SVM
## ### linear
# In[42]:

CV_SVC1=np.array([])
# finding tunning parameter with highest accuracy through cross-validation
for c in range(1,100):
    svc1=SVC(C=c/20,kernel="linear")
    CV_SVC1=np.append(CV_SVC1,cross_val_score(svc1, x_train, y_train, cv=10, scoring='accuracy').mean())
chosen_c_l=np.min(np.where(CV_SVC1>=(CV_SVC1.max()-0.01)))+1
chosen_c_l


# In[43]:

SVC2=SVC(C=chosen_c_l/20,kernel="linear")
SVC2.fit(x_train,y_train)

# In[44]:

y_pred=SVC2.predict(x_test)

# In[45]:

acc_svm_l = accuracy_score(y_test, y_pred)
acc_svm_l


## ### radial

# In[46]:

CV_SVC2=np.array([])
gamma_estimators=np.array([0.01,0.05,0.1,0.15,0.2,0.25,0.3,1/3,0.4,0.5])
# findind best tunning parameter and gamma through cross-validation
for c in range(1,100):
    for g in gamma_estimators:
        svc2=SVC(C=c/20,gamma=g)
        CV_SVC2=np.append(CV_SVC2,cross_val_score(svc2, x_train, y_train, cv=10, scoring='accuracy').mean())


# In[47]:

chosen_c_r=int(np.min(np.where(CV_SVC2>=(CV_SVC2.max()-0.01)))/10)+1
chosen_c_r

# In[48]:

chosen_g=gamma_estimators[((np.min(np.where(CV_SVC2>=(CV_SVC2.max()-0.01))))%10)]
chosen_g

# In[49]:

SVC3=SVC(C=chosen_c_r/20,gamma=chosen_g)
SVC3.fit(x_train,y_train)

# In[50]:

y_pred=SVC3.predict(x_test)


# In[50]:

acc_svm_r = accuracy_score(y_test, y_pred)
acc_svm_r

# In[51]:

CV_SVC3=np.array([])
gamma_estimators=np.array([0.01,0.05,0.1,0.15,0.2,0.25,0.3,1/3,0.4,0.5])
costs20=np.array([0.8*chosen_c_r,0.86*chosen_c_r,0.89*chosen_c_r,0.92*chosen_c_r,0.94*chosen_c_r,0.96*chosen_c_r,0.98*chosen_c_r,0.99*chosen_c_r,chosen_c_r,
                  1.01*chosen_c_r,1.02*chosen_c_r,1.04*chosen_c_r,1.06*chosen_c_r,1.09*chosen_c_r,1.11*chosen_c_r,1.14*chosen_c_r,1.18*chosen_c_r,1.2*chosen_c_r])
# findind best tunning parameter, coefficient, gamma, and degree of polynomial through cross-validation
for c in costs20:
    for d in range(1,7):
        for coef in range(0,5):
                svc3=SVC(C=c/20,gamma=chosen_g,degree=d,kernel="poly",coef0=coef/5,max_iter=2000000)
                CV_SVC3=np.append(CV_SVC3,cross_val_score(svc3, x_train, y_train, cv=6, scoring='accuracy').mean())




# In[52]:

chosen_coef0=int(np.min(np.where(CV_SVC3>=(CV_SVC3.max()-0.01)))%5)+1
chosen_coef0

# In[53]:

chosen_deg=int((np.min(np.where(CV_SVC3>=(CV_SVC3.max()-0.01)))%18)/5)+1
chosen_deg

# In[54]:

chosen_c_p=costs20[int(np.min(np.where(CV_SVC3>=(CV_SVC3.max()-0.01)))/30)]
chosen_c_p

# In[55]:

SVC4=SVC(C=chosen_c_p/20,gamma=chosen_g,degree=chosen_deg,coef0=chosen_coef0/5,kernel="poly")
SVC4.fit(x_train,y_train)

# In[56]:

y_pred=SVC4.predict(x_test)

# In[57]:

acc_svm_p = accuracy_score(y_test, y_pred)
acc_svm_p


## ### Decision Tree Classifier

# In[58]:

Dtc = DecisionTreeClassifier
cv_tree=np.array([])

for i in range(10,500):
    Dtc2=Dtc(min_weight_fraction_leaf=1/((i/10)+1))
    cv_tree=np.append(cv_tree,cross_val_score(Dtc2, x_train, y_train, cv=10,scoring="accuracy").mean())


# In[59]:

chosen_i=np.min(np.where(cv_tree>=(cv_tree.max()-0.01)))+10
chosen_i

# In[60]:

Dtc2=Dtc(min_weight_fraction_leaf=1/((chosen_i/10)+1))
Dtc2.fit(x_train,y_train)

# In[61]:

text_representation = tree.export_text(Dtc2)
print(text_representation)

# In[62]:

with open("decistion_tree.log", "w") as fout:
    fout.write(text_representation)

# In[63]:

fig = plt.figure(figsize=(25,20))
viz_tree = tree.plot_tree(Dtc2, 
                   feature_names=x_train.columns,  
                   class_names=["closed","acquired"],
                   filled=True)



# In[64]:

y_pred=Dtc2.predict(x_test)

# In[65]:

acc_tree = accuracy_score(y_test,y_pred)
acc_tree


## ### Bagging

# In[66]:

cv_bgng=np.array([])
n_estimators=np.array([10,50,100,250,500,1000])
ccp_alpha=np.array([0,0.001,0.005,0.01,0.02,1/((chosen_i/10)+1),0.05,0.1,0.2,0.33])
for i in n_estimators:
    for j in ccp_alpha:
        base = DecisionTreeClassifier(min_weight_fraction_leaf=j)
        bgng = BaggingClassifier(base_estimator = base, 
                          n_estimators = i)
        cv_bgng=np.append(cv_bgng,cross_val_score(bgng, x_train, y_train, cv=6,scoring="accuracy",n_jobs=-1).mean())

# In[67]:

chosen_est1=n_estimators[int(np.min(np.where(cv_bgng>=(cv_bgng.max()-0.01)))/10)]
chosen_est1


# In[68]:

chosen_ccp=ccp_alpha[((np.min(np.where(cv_bgng>=(cv_bgng.max()-0.01))))%10)]
chosen_ccp


# In[69]:

base = DecisionTreeClassifier(min_weight_fraction_leaf=chosen_ccp)
bgng2 = BaggingClassifier(base_estimator = base, 
                          n_estimators = chosen_est1)

# In[71]:

bgng2.fit(x_train, y_train)
y_pred=bgng2.predict(x_test)
acc_bag = accuracy_score(y_test,y_pred)
acc_bag


## ### Random Forest

# In[72]:

cv_rf=np.array([])
new_estimators=np.array([np.array([chosen_est1-200,5]).max(),np.array([chosen_est1-50,8]).max() ,chosen_est1,chosen_est1+100,chosen_est1+250])
mtry_est=np.array([2,5,7,10,12,15,18,20,28,35])
for i in new_estimators:
    for m in mtry_est:

        
        RFC= RandomForestClassifier(n_estimators=i,ccp_alpha=chosen_ccp,max_features=m)
        cv_rf=np.append(cv_rf,cross_val_score(RFC, x_train, y_train, cv=6,scoring="accuracy",n_jobs=-1).mean())
        

# In[73]:

chosen_est2=new_estimators[int(np.min(np.where(cv_rf>=(cv_rf.max()-0.01)))/10)]
chosen_est2

# In[74]:

chosen_mtry=mtry_est[((np.min(np.where(cv_rf>=(cv_rf.max()-0.01))))%10)]
chosen_mtry

# In[75]:

RFC2= RandomForestClassifier(n_estimators=chosen_est2,ccp_alpha=chosen_ccp,max_features=chosen_mtry)
RFC2.fit(x_train, y_train)
y_pred=RFC2.predict(x_test)
acc_rf = accuracy_score(y_test,y_pred)
acc_rf


## ### Gradient Boosting

# In[76]:

cv_gbc=np.array([])
new_estimators=np.array([np.array([chosen_est2-200,5]).max(),np.array([chosen_est2-50,8]).max() ,chosen_est2,chosen_est2+100,chosen_est2+250])

for i in new_estimators:
    for m in range(1,9):
        GBC1= GBC(n_estimators=i,max_depth=m)
        cv_gbc=np.append(cv_gbc,cross_val_score(GBC1, x_train, y_train, cv=10,scoring="accuracy",n_jobs=-1).mean())

# In[77]:

chosen_est3=new_estimators[int(np.min(np.where(cv_gbc>=(cv_gbc.max()-0.01)))/8)]
chosen_est3

# In[78]:

chosen_max=((np.min(np.where(cv_gbc>=(cv_gbc.max()-0.01))))%8)+1
chosen_max

# In[79]:

GBC2= GBC(n_estimators=chosen_est3,max_depth=chosen_max)
GBC2.fit(x_train, y_train)
y_pred=GBC2.predict(x_test)
acc_gb = accuracy_score(y_test,y_pred)
acc_gb


## # Comparing models

## We will use bootstrap to obtain accuracy distribution for all our models. 

# In[80]:

n_iterations = 500
n_size = int(len(x_train) * 0.50)
stats = {'LR': [], 'LDA':  [], 'KNN': [], 'SVM linear': [], 'SVM radial': [], 'SVM polynomial': [], 'TREE': [], 'BAG': [], 'RF': [], 'GB': []}

for i in range(n_iterations):
    train_x, train_y = resample(x_train, y_train, n_samples=n_size)
    model_log = LogisticRegression()
    model_log.fit(train_x, train_y)
    predictions = model_log.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['LR'].append(score)
    
    model_lda = LinearDiscriminantAnalysis()
    model_lda.fit(train_x, train_y)
    predictions = model_lda.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['LDA'].append(score)
      
    model_knn = knn(n_neighbors=chosen_k)
    model_knn.fit(train_x, train_y)
    predictions = model_knn.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['KNN'].append(score)
    
    model_svc=SVC(C=chosen_c_l/20,kernel="linear")
    model_svc.fit(train_x,train_y)
    predictions = model_svc.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['SVM linear'].append(score)

    model_svm_r=SVC(C=chosen_c_r/20,gamma=chosen_g)
    model_svm_r.fit(train_x,train_y)
    predictions = model_svm_r.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['SVM radial'].append(score)
    
    model_svm_p=SVC(C=chosen_c_p/20,gamma=chosen_g,degree=chosen_deg,coef0=chosen_coef0/5,kernel="poly")
    model_svm_p.fit(train_x,train_y)
    predictions = model_svm_p.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['SVM polynomial'].append(score)
    
    model_tree=Dtc(min_weight_fraction_leaf=1/((chosen_i/10)+1))
    model_tree.fit(train_x,train_y)
    predictions = model_tree.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['TREE'].append(score)
    
    model_bag = BaggingClassifier(base_estimator = base, n_estimators = chosen_est1)
    model_bag.fit(train_x,train_y)
    predictions = model_bag.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['BAG'].append(score)
    
    model_rf= RandomForestClassifier(n_estimators=chosen_est2,ccp_alpha=chosen_ccp,max_features=chosen_mtry)
    model_rf.fit(train_x,train_y)
    predictions = model_rf.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['RF'].append(score)
    
    model_gb= GBC(n_estimators=chosen_est3,max_depth=chosen_max)
    model_gb.fit(train_x,train_y)
    predictions = model_gb.predict(x_test)
    score = accuracy_score(y_test, predictions)
    stats['GB'].append(score)


# In[81]:

stats = pd.DataFrame(data=stats)
stats.head()

# In[82]:


alpha = 0.95
lower = list()
upper = list()
for i in stats.columns:
    p = ((1.0-alpha)/2.0) * 100
    lower.append(max(0.0, np.percentile(stats[i], p)))
    p = (alpha+((1.0-alpha)/2.0)) * 100
    upper.append(min(1.0, np.percentile(stats[i], p)))

# In[83]:

d = {'test accuracy': [acc_log, acc_lda, acc_knn, acc_svm_l, acc_svm_r, acc_svm_p, acc_tree, acc_bag, acc_rf, acc_gb], 'lower bound': lower, 'upper bound' : upper}
accuracy  = pd.DataFrame(data=d, index=['logistic regression', 'LDA','K-NN', 'SVM linear', 'SVM radial', 'SVM polynomial', 'Decidion tree','Bagging', 'Random Forest', 'Gradient Boosting'])
accuracy


# In[84]:

figure(figsize=(12, 10))
ax = sns.boxplot(x="variable", y="value", data=pd.melt(stats))


## We can see from boxplot that Random Forest, Bagging and Gradient Boosting peformend best in this case. It can be because all other model are not flexible enough, like LDA or logistic regression.

